{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/docker_logo_1.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably fair to say that access to servers has never been easier.  With platforms such as AWS, Azure, and Google GCE we can now launch on-demand servers of all varieties and configurations.  This programable infrastructure (IaaS) help companies, agencies, and institutions maintain agility as market and mission pressures evolve. However, even with the rise of IaaS, application packaging, configuration, and composition has not advanced despite considerable efforts in configuration management.  This is where [`docker`](https://www.docker.com/) comes in.  \n",
    "\n",
    "`Docker` is not about full virtualization but rather about the ease of packaging and running applications using [Linux containers](https://en.wikipedia.org/wiki/LXC).  The idea is that `docker` containers wrap a piece of software or application in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries (i.e. anything that can be installed on a server). This guarantees that the software will always run the same everywhere, regardless of the OS/compute environment running the container. Docker also provides portable Linux deployment such that containers can be run on any Linux system with kernel is 3.10 or later.  All major Linux distros have supported Docker since 2014.  While no doubt containers and virtual machines have similar resource isolation and allocation benefits, the architectural approach of Linux containers allows containerized applications to be more portable and efficient.\n",
    "\n",
    "At NVIDIA, we use containers in a variety of ways including development, testing, benchmarking, and of course in production as the mechanism for deploying deep learning frameworks. Using [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker), a light-weight `docker` plugin, we can develop and prototype GPU applications on a workstation, and then deploy those applications anywhere that supports GPU containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In the interest of time, we've already configured docker and nvidia-docker. If you're interested in setup details, see Appendix A at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Contact\n",
    "\n",
    "The simplest way to interact with `docker` is probably to just ask for the version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 1.12.3, build 6b644ec\r\n"
     ]
    }
   ],
   "source": [
    "docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask `nvidia-docker` for the version information too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 1.12.3, build 6b644ec\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `nvidia-docker` invocation here was simply \"pass through\" to `docker` command itself.\n",
    "\n",
    "Next best way to get familiar with docker command line is to ask for `--help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: docker [OPTIONS] COMMAND [arg...]\r\n",
      "       docker [ --help | -v | --version ]\r\n",
      "\r\n",
      "A self-sufficient runtime for containers.\r\n",
      "\r\n",
      "Options:\r\n",
      "\r\n",
      "  --config=~/.docker              Location of client config files\r\n",
      "  -D, --debug                     Enable debug mode\r\n",
      "  -H, --host=[]                   Daemon socket(s) to connect to\r\n",
      "  -h, --help                      Print usage\r\n",
      "  -l, --log-level=info            Set the logging level\r\n",
      "  --tls                           Use TLS; implied by --tlsverify\r\n",
      "  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\r\n",
      "  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\r\n",
      "  --tlskey=~/.docker/key.pem      Path to TLS key file\r\n",
      "  --tlsverify                     Use TLS and verify the remote\r\n",
      "  -v, --version                   Print version information and quit\r\n",
      "\r\n",
      "Commands:\r\n",
      "    attach    Attach to a running container\r\n",
      "    build     Build an image from a Dockerfile\r\n",
      "    commit    Create a new image from a container's changes\r\n",
      "    cp        Copy files/folders between a container and the local filesystem\r\n",
      "    create    Create a new container\r\n",
      "    diff      Inspect changes on a container's filesystem\r\n",
      "    events    Get real time events from the server\r\n",
      "    exec      Run a command in a running container\r\n",
      "    export    Export a container's filesystem as a tar archive\r\n",
      "    history   Show the history of an image\r\n",
      "    images    List images\r\n",
      "    import    Import the contents from a tarball to create a filesystem image\r\n",
      "    info      Display system-wide information\r\n",
      "    inspect   Return low-level information on a container, image or task\r\n",
      "    kill      Kill one or more running containers\r\n",
      "    load      Load an image from a tar archive or STDIN\r\n",
      "    login     Log in to a Docker registry.\r\n",
      "    logout    Log out from a Docker registry.\r\n",
      "    logs      Fetch the logs of a container\r\n",
      "    network   Manage Docker networks\r\n",
      "    node      Manage Docker Swarm nodes\r\n",
      "    pause     Pause all processes within one or more containers\r\n",
      "    port      List port mappings or a specific mapping for the container\r\n",
      "    ps        List containers\r\n",
      "    pull      Pull an image or a repository from a registry\r\n",
      "    push      Push an image or a repository to a registry\r\n",
      "    rename    Rename a container\r\n",
      "    restart   Restart a container\r\n",
      "    rm        Remove one or more containers\r\n",
      "    rmi       Remove one or more images\r\n",
      "    run       Run a command in a new container\r\n",
      "    save      Save one or more images to a tar archive (streamed to STDOUT by default)\r\n",
      "    search    Search the Docker Hub for images\r\n",
      "    service   Manage Docker services\r\n",
      "    start     Start one or more stopped containers\r\n",
      "    stats     Display a live stream of container(s) resource usage statistics\r\n",
      "    stop      Stop one or more running containers\r\n",
      "    swarm     Manage Docker Swarm\r\n",
      "    tag       Tag an image into a repository\r\n",
      "    top       Display the running processes of a container\r\n",
      "    unpause   Unpause all processes within one or more containers\r\n",
      "    update    Update configuration of one or more containers\r\n",
      "    version   Show the Docker version information\r\n",
      "    volume    Manage Docker volumes\r\n",
      "    wait      Block until a container stops, then print its exit code\r\n",
      "\r\n",
      "Run 'docker COMMAND --help' for more information on a command.\r\n"
     ]
    }
   ],
   "source": [
    "docker --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of `docker` command line interactions is: \n",
    "\n",
    "`docker [OPTIONS] COMMAND [arg...]` \n",
    "\n",
    "and as the help display shows there are a lot of commands to choose from.  Don't worry, much like a big city, once we become more familiar with these commands the list won't seem so big.  We can start to drill down and get help that is specific to each command.  For example, one of the most useful `docker` commands is **`images`** which list all local containers on the host that `docker` knows about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:\tdocker images [OPTIONS] [REPOSITORY[:TAG]]\r\n",
      "\r\n",
      "List images\r\n",
      "\r\n",
      "Options:\r\n",
      "  -a, --all             Show all images (default hides intermediate images)\r\n",
      "      --digests         Show digests\r\n",
      "  -f, --filter value    Filter output based on conditions provided (default [])\r\n",
      "      --format string   Pretty-print images using a Go template\r\n",
      "      --help            Print usage\r\n",
      "      --no-trunc        Don't truncate output\r\n",
      "  -q, --quiet           Only show numeric IDs\r\n"
     ]
    }
   ],
   "source": [
    "docker images --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, lets now ask `docker` about the what container images are available locally on the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        8 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        8 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the output specifies three container images with some general metadata associated with each one.  First you'll notice that the images are quite large on average (~ 2GB) and that each image is associated with a unique ID hash.  When containers are created (i.e. via the **`create`** command) they are created from images.  There is no limit to the number of containers we can create from an images so it is important that `docker` associates UUIDs for each image and container.  Notice the `REPOSITORY` and `TAG` columns here specify more human readable image labels.  The repository loosely coresponds to the image name (i.e. url) and just as in the version control system [GIT](https://git-scm.com/) images can be modified and \"tagged\" rather than explicitly changing the image name for each image version. \n",
    "\n",
    "Here we have the \"`nvidia/cuda`\" container with ID `c54a2cc56cbb` and is tagged as the \"`latest`\" version of the image (i.e. most current).  The deep learning library [`cuDNN`](https://developer.nvidia.com/cudnn) was added to the image and a new image was created under the same name but tagged appropriately as \"`8.0-cudnn5-devel`\".\n",
    "\n",
    "You're probably wondering already \"*where does docker store these containers?*\".  In general, docker works in `/var/lib/docker` and images are stored in `image` subdirectory.  For more information and details about where and how docker stores images on the host machine, see [here](https://stackoverflow.com/questions/19234831/where-are-docker-images-stored-on-the-host-machine#). \n",
    "\n",
    "For now just know that docker works with \"images\" and all containers are created from these images.  We will go into all the details about creating and modifying images etc in just a bit.  But first, lets actually kick around some containers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first lets have docker list *all* containers using the **`ps`** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are probably no containers listed which is fine because we're going to create some containers from images next.  Again, don't forget you can get help for each command with `docker [COMMAND] --help`.  Use this to get additional details on the **`ps`** command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now use the docker command **`create`** to initialize a container from the `nvidia/cuda:latest` image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41524c54cf1c5e4370fbee2d2a6f5965290f59e61e3a8f52e5d3c9a5284d5001\r\n"
     ]
    }
   ],
   "source": [
    "docker create nvidia/cuda:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responce we recieved is a sha256 UUID for the generated container and listing `docker` containers again we see this new container now listed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS               NAMES\r\n",
      "41524c54cf1c        nvidia/cuda:latest   \"/bin/bash\"         6 seconds ago       Created                                 fervent_kalam\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that the container is not actually doing anything right now.  We've only \"stamped\" out a container from an image -- the container is not running at this point.  Were the container active the `STATUS` would read \"running\".  OK, so what is the container doing there?  Well the answer is \"nothing\".  Think about when we enter commands on the command-line -- each time we hit enter we implicitly specify that we would like that command to be executed immediately.  You can think of containers as a command that has not yet executed.  This command is wrapped up in the container and has all the resources (libraries etc) needed for successful execution.  Speaking of which, lets actually run this container ... \n",
    "\n",
    "Using the 12 character container id provided by the **`docker ps -a`** command above we can run the container as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\r\n",
      "Pulling repository docker.io/library/41524c54cf1c\r\n",
      "nvidia-docker | 2016/12/16 20:59:56 Error: image library/41524c54cf1c:latest not found\r\n"
     ]
    }
   ],
   "source": [
    "# copy your CONTAINER ID from the docker ps -a command above\n",
    "nvidia-docker run 41524c54cf1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hm ... we got an ERROR.  Using the **`run`** command seemed like a good guess!  Why does the **`run`** command not work here?  More on this later in the next few cells.  \n",
    "\n",
    "Lets try the **`start`** command instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41524c54cf1c\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start 41524c54cf1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks better. Using the **`start`** command docker returned the sha256 UUID.  Lets have a look at the docker containers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS                     PORTS               NAMES\r\n",
      "41524c54cf1c        nvidia/cuda:latest   \"/bin/bash\"         5 minutes ago       Exited (0) 3 seconds ago                       fervent_kalam\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the status says \"Exited (0) ...\".  Notice that the command (i.e. entry point) is `/bin/bash`.  When the **`start`** was issued the \"COMMAND\" was executed and by definition *bash command language interpreter that executes commands read from the standard input or from a file*.  However, there were no commands to execute from standard input!  Containers can have other entry points -- the reason `/bin/bash` is used most often is that it allows the container to act more generically as a shell so we can send it additional instructions. Note that all containers have a default entrypoint of \"`/bin/sh -c`\" unless otherwise specified.\n",
    "\n",
    "Here our hands are tied with what the container will do.  Each time we issue the **`start`** command the container executes the entrypoint \"`/bin/bash`\" and since there is nothing on standard input the container simply exits.  This is where the **`run`** command comes in.\n",
    "\n",
    "Instead of creating and starting a container explicitly we can use the **`run`** command to exectue a command within a particular image via creating a container from that image with the appropriate entrypoint.  Lets issue a **`run`** command passing the image ID of the \"`nvidia/cuda:latest`\" image as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# don't forget to use the container id from the \"docker ps -a\" command\n",
    "nvidia-docker run 367795fb1051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the command **`start`** takes a container ID as the argument while the **`run`** command takes an image ID.  Lets have a look at the containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "c0d4c1b7ec22        367795fb1051         \"/bin/bash\"         6 seconds ago       Exited (0) 5 seconds ago                        prickly_almeida\r\n",
      "41524c54cf1c        nvidia/cuda:latest   \"/bin/bash\"         5 minutes ago       Exited (0) 40 seconds ago                       fervent_kalam\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an additional container both from image `367795fb1051` which have exited.  \n",
    "\n",
    "At this point the **`run`** command has done exactly what the **`start`** command has done (i.e. started a container which executed the entrypoint and exited).  However, the docker **`run`** command allows us to pass an alternative command to the container (`docker run --help`).  Lets try to pass an alternative instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 16 21:01:03 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   23C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run 367795fb1051 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally!  Just to be clear, the `nvidia-smi` command was exectued within the container -- not the host.  Lets have a look at the containers yet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS                          PORTS               NAMES\r\n",
      "9cedbf73f11d        367795fb1051         \"nvidia-smi\"        10 seconds ago      Exited (0) 8 seconds ago                            sleepy_leavitt\r\n",
      "c0d4c1b7ec22        367795fb1051         \"/bin/bash\"         36 seconds ago      Exited (0) 35 seconds ago                           prickly_almeida\r\n",
      "41524c54cf1c        nvidia/cuda:latest   \"/bin/bash\"         6 minutes ago       Exited (0) About a minute ago                       fervent_kalam\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new container from image `367795fb1051` but the \"COMMAND\" has been set to `nvidia-smi` as instructed by our **`run`** command.  Just for kicks lets issue a **`start`** command to this new container.  Each container gets a uuid that will change every time this lab is run so make sure to replace the container ID in the command below with the appropriate container ID listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9cedbf73f11d\r\n"
     ]
    }
   ],
   "source": [
    "docker start 9cedbf73f11d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wait a minute, where is our output??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED              STATUS                          PORTS               NAMES\r\n",
      "9cedbf73f11d        367795fb1051         \"nvidia-smi\"        About a minute ago   Exited (0) 4 seconds ago                            sleepy_leavitt\r\n",
      "c0d4c1b7ec22        367795fb1051         \"/bin/bash\"         About a minute ago   Exited (0) About a minute ago                       prickly_almeida\r\n",
      "41524c54cf1c        nvidia/cuda:latest   \"/bin/bash\"         7 minutes ago        Exited (0) 2 minutes ago                            fervent_kalam\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough when we check the docker container status it has status of \"Exited (0) 10 seconds ago ...\" which means that the **`start`** command did indeed start the container. The long story short is that the **`run`** command automatically provides the standard output from the command specified where as **`start`** does not forward the stdout by default -- we have to explicitly ask.  According to the help section for the **`start`** command, the option \"`--attach`\" attaches STDOUT/STDERR and forward signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 16 21:02:19 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   23C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "docker start --attach 9cedbf73f11d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo!  We got the output form the command using `--attach` option when using the **`start`** command.  We can get the associated help for the attach option to see that indeed we get STDOUT/STDERR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:\tdocker start [OPTIONS] CONTAINER [CONTAINER...]\r\n",
      "\r\n",
      "Start one or more stopped containers\r\n",
      "\r\n",
      "Options:\r\n",
      "  -a, --attach               Attach STDOUT/STDERR and forward signals\r\n",
      "      --detach-keys string   Override the key sequence for detaching a container\r\n",
      "      --help                 Print usage\r\n",
      "  -i, --interactive          Attach container's STDIN\r\n"
     ]
    }
   ],
   "source": [
    "docker start --help "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable to ask where STDOUT goes when not attached?  The answer is that STDOUT/STDERR are piped to the container *log* file.  Each container has a log file associated with it which can be accessed using the **`logs`** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 16 21:01:03 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   23C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "Fri Dec 16 21:02:06 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   23C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "Fri Dec 16 21:02:19 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   23C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "docker logs 9cedbf73f11d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few final words on starting and running containers. Keep an eye out on the container list when using the **`run`** command as each invocation creates a *new* image.  There is no problem having many (many) container stitting around but eventually it creates clutter.  Remember, containers are ment to be light-weight and disposable.  To that end lets clean up our containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9cedbf73f11d\r\n",
      "c0d4c1b7ec22\r\n",
      "41524c54cf1c\r\n"
     ]
    }
   ],
   "source": [
    "# generate a list of container ID from the docker ps command\n",
    "docker ps -a | awk '{print $1}' | tail -n +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9cedbf73f11d\r\n",
      "c0d4c1b7ec22\r\n",
      "41524c54cf1c\r\n"
     ]
    }
   ],
   "source": [
    "# for each container ID use the docker \"rm\" command to remove/delete the container\n",
    "for cid in $(docker ps -a | awk '{print $1}' | tail -n +2);do docker rm $cid; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All cleaned up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the **`run`** command creates a new container each time and using the docker **`ps`** command we can see each new container as we run commands.  However this can get combersom to have to manually clean out containers all the time. The solution to this is to use the **`--rm`** option with the **`run`** command.  This instructs docker to simply remove the container after execution.  This is quite convenient for most situations.  Keep in mind however, that once the container has been deleted it can not be started again etc -- it's gone.  In general this is the desired workflow since containers are intended to be light-weight disposible execution units.  After all, if you need the container again, no problem, just create another one! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "So far we have discussed what docker is (i.e. virtual machines v.s. Linux containers) and how to view (**`ps`**), **`create`**, **`start`**, **`run`**, and **`rm`** containers created from `docker` images.  Furthermore we've investigated various options associated with these `docker` commands such as `--attach` and `--rm` and familiarized ourselves with how to obtain help for `docker` and each of the `docker` commands.\n",
    "\n",
    "### Exercises\n",
    "Make sure that you're comfortable creating containers and executing commands before moving forward.  Docker is quite forgiving so do be afraid to try lots of different things out while you explore.  Here are a few suggestions: \n",
    "0. Try launching containers from the other images\n",
    "1. Try issuing the **`run`** command with the option `--rm` and confirm the continer is cleaned up \n",
    "2. It is often useful to give our containers a name, try this with the `--name` option with the **`run`** command\n",
    "3. Have the container execute `whoami`.  Think about what user might get returned before you run this.\n",
    "4. Maybe try `ifconfig` inside of the container.  Is the MAC address the same every time?\n",
    "5. Get the container to ping google (pro tip: use option `-c1` so you don't ping forever) \n",
    "6. What does the container return when asked for disk usage (i.e `df -h`)??\n",
    "7. Use the `--env` option with the **`run`** command to set environment variables AWS_S3_BUCKET, AWS_ACCESS_KEY, and AWS_SECRET_KEY\n",
    "8. Notice the containers maintain state. Verify this by touching a file and then running the container again\n",
    "\n",
    "\n",
    "#### Food for Thought: \n",
    "What happens when you execute `rm -rf /` inside a container?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving Deeper into Images\n",
    "By now you probably comfortable with launching containers with `docker` from the images that were already available when we started.  The next step is understanding how to manage your own images.  This includes things like importing images into `docker`, modifying existing images, exporting images, and of course deleting images. \n",
    "\n",
    "In the `docker` world most images have a \"parent\".  This means that the image was created by modifying some existing image.  In general, this is the typical workflow in `docker`.  The idea is that it is easy to create images and lets just reuse what's already existing so as to be most efficient.  \n",
    "\n",
    "However, there is an essential difference when working with `docker` images. In the virtual machine world, you modify a 4 GB machine image and then do \"save as\" and create a new 4 GB machine image that contains your changes/updates.  In this way virtual machine images are totally independent but very heavy weight. \n",
    "\n",
    "When we make modification to an existing `docker` image and use this to create our own \"new\" image, `docker` does not store two images.  Just a the GIT version control does not make a new copy of a file everytime a modification is commited, so too `docker` works with images in \"layers\" so that changes or modifications to an image are stored as a new light-weight image deltas called \"layers\".  In this way we can take an existing 2 GB `docker` image and create 10 new images each with a few modifications of this base image without having to store 20 GB of new images!  That is, in creating a new docker image from a parent, we only have to store the changes to the parent image.\n",
    "\n",
    "As you might imagine, only having to store deltas to images allows for many many (many!) images to be generated without having to pay the full cost of having all those images around.  Therefore in the `docker` world, images abound since it is efficient and light-weight to generate new images from an existing parent image. Luckily, `docker` provides ways to manage all these images using \"image repositories\" so that we can manage images just like we would version controled files in a `git` repository.  More on `docker` repositories later.\n",
    "\n",
    "### New Image from Container Modifications\n",
    "Lets first update the existing `nvidia/cuda` image by creating a container that executes `apt-get update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        8 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        8 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ign http://archive.ubuntu.com trusty InRelease\r\n",
      "Get:1 http://archive.ubuntu.com trusty-updates InRelease [65.9 kB]\r\n",
      "Get:2 http://archive.ubuntu.com trusty-security InRelease [65.9 kB]\r\n",
      "Ign http://developer.download.nvidia.com  InRelease\r\n",
      "Get:3 http://developer.download.nvidia.com  Release.gpg [819 B]\r\n",
      "Get:4 http://developer.download.nvidia.com  Release [564 B]\r\n",
      "Get:5 http://archive.ubuntu.com trusty Release.gpg [933 B]\r\n",
      "Get:6 http://archive.ubuntu.com trusty Release [58.5 kB]\r\n",
      "Get:7 http://archive.ubuntu.com trusty-updates/main Sources [480 kB]\r\n",
      "Get:8 http://developer.download.nvidia.com  Packages [107 kB]\r\n",
      "Get:9 http://archive.ubuntu.com trusty-updates/restricted Sources [5921 B]\r\n",
      "Get:10 http://archive.ubuntu.com trusty-updates/universe Sources [214 kB]\r\n",
      "Get:11 http://archive.ubuntu.com trusty-updates/main amd64 Packages [1161 kB]\r\n",
      "Get:12 http://archive.ubuntu.com trusty-updates/restricted amd64 Packages [20.4 kB]\r\n",
      "Get:13 http://archive.ubuntu.com trusty-updates/universe amd64 Packages [505 kB]\r\n",
      "Get:14 http://archive.ubuntu.com trusty-security/main Sources [157 kB]\r\n",
      "Get:15 http://archive.ubuntu.com trusty-security/restricted Sources [4621 B]\r\n",
      "Get:16 http://archive.ubuntu.com trusty-security/universe Sources [54.9 kB]\r\n",
      "Get:17 http://archive.ubuntu.com trusty-security/main amd64 Packages [700 kB]\r\n",
      "Get:18 http://archive.ubuntu.com trusty-security/restricted amd64 Packages [17.0 kB]\r\n",
      "Get:19 http://archive.ubuntu.com trusty-security/universe amd64 Packages [191 kB]\r\n",
      "Get:20 http://archive.ubuntu.com trusty/main Sources [1335 kB]\r\n",
      "Get:21 http://archive.ubuntu.com trusty/restricted Sources [5335 B]\r\n",
      "Get:22 http://archive.ubuntu.com trusty/universe Sources [7926 kB]\r\n",
      "Get:23 http://archive.ubuntu.com trusty/main amd64 Packages [1743 kB]\r\n",
      "Get:24 http://archive.ubuntu.com trusty/restricted amd64 Packages [16.0 kB]\r\n",
      "Get:25 http://archive.ubuntu.com trusty/universe amd64 Packages [7589 kB]\r\n",
      "Fetched 22.4 MB in 5s (4246 kB/s)\r\n",
      "Reading package lists...\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run 367795fb1051 apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, since we did not use the `--rm` option our container is still available -- which is what we want since we're going to create a new image from this updated container. Notice that we can not save changes to a container that has been removed/deleted.  This should be obvious but just saying ... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "938e7acea158        367795fb1051        \"apt-get update\"    26 seconds ago      Exited (0) 12 seconds ago                       cranky_bartik\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `Docker` lets us keep these changes by committing them into a new image.  Under the hood, `docker` keeps track of the differenced between the base image (`nvidia/cuda` or rather `367795fb1051`) by creating a new image layer using the union filesystem ([UnionFS](https://en.wikipedia.org/wiki/UnionFS)).  To see this, we can inspect the changes to the container using the `docker` **`diff`** command which takes a the container ID as an argument.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C /usr\r\n",
      "C /usr/local\r\n",
      "A /usr/local/nvidia\r\n",
      "C /var\r\n",
      "C /var/lib\r\n",
      "C /var/lib/apt\r\n",
      "C /var/lib/apt/lists\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_universe_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_restricted_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_universe_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_restricted_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_InRelease\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_main_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_Release\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_main_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/developer.download.nvidia.com_compute_cuda_repos_ubuntu1404_x86%5f64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_InRelease\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_universe_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_Release.gpg\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_restricted_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/developer.download.nvidia.com_compute_cuda_repos_ubuntu1404_x86%5f64_Release.gpg\r\n",
      "A /var/lib/apt/lists/lock\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_restricted_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_restricted_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_universe_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-updates_main_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_main_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/partial\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_main_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_restricted_binary-amd64_Packages.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_universe_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty_universe_source_Sources.gz\r\n",
      "A /var/lib/apt/lists/developer.download.nvidia.com_compute_cuda_repos_ubuntu1404_x86%5f64_Release\r\n",
      "A /var/lib/apt/lists/archive.ubuntu.com_ubuntu_dists_trusty-security_main_source_Sources.gz\r\n"
     ]
    }
   ],
   "source": [
    "# make sure to use container id from \"docker ps -a\" command above\n",
    "docker diff 938e7acea158"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where **`A`** means that the file or directory listed was added, **`C`** means created and **`D`** means deleted\n",
    "\n",
    "Lets now use the `docker` **`commit`** command to generate a new image from this container.  You might want to check your disk usage before and after image creation just to verify for yourself that the new image does not eat up an additional 2 GB of disk space on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "udev            7.4G   12K  7.4G   1% /dev\r\n",
      "tmpfs           1.5G  800K  1.5G   1% /run\r\n",
      "/dev/xvda1       32G  8.4G   22G  28% /\r\n",
      "none            4.0K     0  4.0K   0% /sys/fs/cgroup\r\n",
      "none            5.0M     0  5.0M   0% /run/lock\r\n",
      "none            7.4G     0  7.4G   0% /run/shm\r\n",
      "none            100M   16K  100M   1% /run/user\r\n"
     ]
    }
   ],
   "source": [
    "df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:5980494cc2123a15bb4ed8f5393d847332a791f23fa237bfc37975bc328d3921\r\n"
     ]
    }
   ],
   "source": [
    "# make sure to use the appropriate container ID here\n",
    "docker commit <CONTAINER-ID> newiamgename:update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here using the `docker` **`commit`** command we provided the unique container ID as provided by the **`ps`** command and a new name:tag for the resulting image.  Now lets list the `docker` images and we should see our new image there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "newiamgename        update              5980494cc212        5 seconds ago       1.637 GB\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        5 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        5 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice that the image size says something like 1.6 GB.  Verify with `df` that we have not actually used additional physical space on host disk in generating this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "udev            7.4G   12K  7.4G   1% /dev\r\n",
      "tmpfs           1.5G  800K  1.5G   1% /run\r\n",
      "/dev/xvda1       32G  8.4G   22G  28% /\r\n",
      "none            4.0K     0  4.0K   0% /sys/fs/cgroup\r\n",
      "none            5.0M     0  5.0M   0% /run/lock\r\n",
      "none            7.4G     0  7.4G   0% /run/shm\r\n",
      "none            100M   16K  100M   1% /run/user\r\n"
     ]
    }
   ],
   "source": [
    "df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOTCHA**: `docker` does not allow upper case characters in the image names and doing so generates the error message: \"invalid reference format\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gernerating Tar Files for Sharing\n",
    "There are two `docker` commands for creating a `tar` file that can be shared with others.  The first is that we can use the `docker` commands **`save`** and **`load`** to create and ingest *image* tar files.  The second option is to use the `docker` commands **`export`** and **`import`** to create and ingest *container* tar files\n",
    "\n",
    "Notice the `help` definitions for each set of commands:\n",
    "\n",
    "|  | When working with Containers   |\n",
    "|------|------|\n",
    "| **`export`** | Export a container's filesystem as a tar archive                         |\n",
    "| **`import`** | Import the contents from a tarball to create a filesystem image |\n",
    "|              | **When working with Images** |\n",
    "| **`save`**   | Save one or more images to a tar archive (streamed to STDOUT by default)   |\n",
    "| **`load`**   | Load an image from a tar archive or STDIN|\n",
    "\n",
    "Lets save the new image created in the previous section as a tar-ball on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "docker save -o dockerimageexport.tar newiamgename:update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look in our current working directory we should see a nice fat tar-ball of our `docker` image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 ubuntu ubuntu 1.6G Nov 30 18:31 \u001b[0m\u001b[01;31mdockerimageexport.tar\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls -lah dockerimageexport.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that when an image is saved to the host file system the full size of the image is physically allocated. We can see this here as the file `dockerimageexport.tar` has size 1.6G.\n",
    "\n",
    "Lets now remove our new image we just commited from `docker` using the **`rmi`** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untagged: newiamgename:update\r\n",
      "Deleted: sha256:5980494cc2123a15bb4ed8f5393d847332a791f23fa237bfc37975bc328d3921\r\n",
      "Deleted: sha256:9a5db72eda6a4bebd250ce33ee0e6c24866e8f519de2c34c2e248056fed3ca8d\r\n"
     ]
    }
   ],
   "source": [
    "docker rmi 5980494cc212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our `docker` images again we no longer see newimagename:update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        5 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        5 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load the saved image into docker using the **`load`** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[0A\u001b[2K\r",
      "607a3d572caf: Loading layer 229.4 kB/22.43 MB\r",
      "\u001b[0B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 458.8 kB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 688.1 kB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 917.5 kB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 1.147 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 1.376 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 1.606 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 1.835 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 2.064 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 2.294 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 2.523 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 2.753 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 2.982 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 3.211 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 3.441 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  3.67 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 3.899 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 4.129 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 4.358 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 4.588 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 4.817 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 5.046 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 5.276 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 5.505 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 5.734 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 5.964 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 6.193 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 6.423 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 6.652 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 6.881 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 7.111 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  7.34 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 7.569 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 7.799 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 8.028 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 8.258 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 8.487 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 8.716 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 8.946 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 9.175 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 9.404 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 9.634 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 9.863 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 10.09 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 10.32 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 10.55 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 10.78 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 11.01 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 11.24 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 11.47 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  11.7 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 11.93 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 12.16 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 12.39 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 12.62 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 12.85 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 13.07 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  13.3 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 13.53 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 13.76 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 13.99 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 14.22 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 14.45 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 14.68 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 14.91 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 15.14 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 15.37 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  15.6 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 15.83 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 16.06 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 16.29 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 16.52 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 16.74 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 16.97 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  17.2 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 17.43 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 17.66 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 17.89 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 18.12 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 18.35 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 18.58 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 18.81 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 19.04 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 19.27 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  19.5 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 19.73 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 19.96 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 20.19 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 20.41 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 20.64 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 20.87 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer  21.1 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 21.33 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 21.56 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 21.79 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 22.02 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 22.25 MB/22.43 MB\r",
      "\u001b[1B\u001b[1A\u001b[2K\r",
      "607a3d572caf: Loading layer 22.43 MB/22.43 MB\r",
      "\u001b[1BLoaded image: newiamgename:update\r\n"
     ]
    }
   ],
   "source": [
    "docker load --input dockerimageexport.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "newiamgename        update              5980494cc212        4 minutes ago       1.637 GB\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        5 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        5 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the image `newimagename:update` listed!  \n",
    "\n",
    "A few final words on saving vs exporting.  While the two methods are indeed similar in functionality, the difference is that *saving* an image will keep its history (i.e. all parent layers, tags, and versions) while *exporting* a container will squash its history producing a flattened single layer resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Images with Dockerfiles\n",
    "Launching containers, making updates, and commiting the changes does work well however, it is quite manual.  To automate this image construction workflow `docker` provides a manifesto, called a [`dockerfile`](https://docs.docker.com/engine/reference/builder/), which is a text file that lists the build steps.  Dockerfiles are quite popular in the `docker` community and often docker files are exchanged rather than image tar-balls. While simple, there are a few common pitfalls when createing dockerfiles.  Read up on the [dockerfile best practices](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/) for some excellent pointers that will save you lots of time.\n",
    "\n",
    "Dockerfiles are quite simple lets create a dockerfile to build and updated version of the `nvidia/cuda:latest` image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cat << LINES > Dockerfile\n",
    "FROM nvidia/cuda:latest\n",
    "RUN apt-get update\n",
    "ENTRYPOINT [\"/bin/sh\", \"-c\"]\n",
    "CMD [\"nvidia-smi\"]\n",
    "LINES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`FROM`](https://docs.docker.com/engine/reference/builder/#from) instruction sets the base image for subsequent instructions. As such, a valid `dockerfile` must have `FROM` as its first instruction. The image can be any valid image.  The [`RUN`](https://docs.docker.com/engine/reference/builder/#run) instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the `dockerfile`.  Finally, the main purpose of a [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd) is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint) instruction as well.  For more information on how `CMD` and `ENTRYPOINT` interact see [here](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).\n",
    "\n",
    "To build an image using this `dockerfile` we invoke the `docker` **`build`** command (this takes about 60 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon 506.9 kB\r",
      "\r",
      "\r\n",
      "Step 1 : FROM nvidia/cuda:latest\r\n",
      " ---> 367795fb1051\r\n",
      "Step 2 : RUN apt-get update\r\n",
      " ---> Running in 90a83491d019\r\n",
      "Ign http://archive.ubuntu.com trusty InRelease\r\n",
      "Get:1 http://archive.ubuntu.com trusty-updates InRelease [65.9 kB]\r\n",
      "Ign http://developer.download.nvidia.com  InRelease\r\n",
      "Get:2 http://developer.download.nvidia.com  Release.gpg [819 B]\r\n",
      "Get:3 http://developer.download.nvidia.com  Release [564 B]\r\n",
      "Get:4 http://archive.ubuntu.com trusty-security InRelease [65.9 kB]\r\n",
      "Get:5 http://archive.ubuntu.com trusty Release.gpg [933 B]\r\n",
      "Get:6 http://archive.ubuntu.com trusty-updates/main Sources [480 kB]\r\n",
      "Get:7 http://developer.download.nvidia.com  Packages [107 kB]\r\n",
      "Get:8 http://archive.ubuntu.com trusty-updates/restricted Sources [5921 B]\r\n",
      "Get:9 http://archive.ubuntu.com trusty-updates/universe Sources [214 kB]\r\n",
      "Get:10 http://archive.ubuntu.com trusty-updates/main amd64 Packages [1161 kB]\r\n",
      "Get:11 http://archive.ubuntu.com trusty-updates/restricted amd64 Packages [20.4 kB]\r\n",
      "Get:12 http://archive.ubuntu.com trusty-updates/universe amd64 Packages [505 kB]\r\n",
      "Get:13 http://archive.ubuntu.com trusty Release [58.5 kB]\r\n",
      "Get:14 http://archive.ubuntu.com trusty-security/main Sources [157 kB]\r\n",
      "Get:15 http://archive.ubuntu.com trusty-security/restricted Sources [4621 B]\r\n",
      "Get:16 http://archive.ubuntu.com trusty-security/universe Sources [54.9 kB]\r\n",
      "Get:17 http://archive.ubuntu.com trusty-security/main amd64 Packages [700 kB]\r\n",
      "Get:18 http://archive.ubuntu.com trusty-security/restricted amd64 Packages [17.0 kB]\r\n",
      "Get:19 http://archive.ubuntu.com trusty-security/universe amd64 Packages [191 kB]\r\n",
      "Get:20 http://archive.ubuntu.com trusty/main Sources [1335 kB]\r\n",
      "Get:21 http://archive.ubuntu.com trusty/restricted Sources [5335 B]\r\n",
      "Get:22 http://archive.ubuntu.com trusty/universe Sources [7926 kB]\r\n",
      "Get:23 http://archive.ubuntu.com trusty/main amd64 Packages [1743 kB]\r\n",
      "Get:24 http://archive.ubuntu.com trusty/restricted amd64 Packages [16.0 kB]\r\n",
      "Get:25 http://archive.ubuntu.com trusty/universe amd64 Packages [7589 kB]\r\n",
      "Fetched 22.4 MB in 4s (4928 kB/s)\r\n",
      "Reading package lists...\r\n",
      " ---> 049dd8e11f2a\r\n",
      "Removing intermediate container 90a83491d019\r\n",
      "Step 3 : ENTRYPOINT /bin/sh -c\r\n",
      " ---> Running in 1b85fabf673c\r\n",
      " ---> 7038f1e7a8f8\r\n",
      "Removing intermediate container 1b85fabf673c\r\n",
      "Step 4 : CMD nvidia-smi\r\n",
      " ---> Running in 4a010da2ff72\r\n",
      " ---> 704bbf8f9727\r\n",
      "Removing intermediate container 4a010da2ff72\r\n",
      "Successfully built 704bbf8f9727\r\n"
     ]
    }
   ],
   "source": [
    "docker build -t foo:bar ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "foo                 bar                 704bbf8f9727        17 seconds ago      1.637 GB\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        8 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        8 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the new image built with the dockerfile.  Notice that we used the `-t` option when building the Dockerfile so that we could provide a `REPOSITORY` and `TAG`.  Without the `-t` option both repository and tag would be set to \"`<none>`\".  This is not necessarily a problem, it just means that you will have have no other option but to reference the image using the `IMAGE ID`. Don't hesitate to use the **`rmi`** command to clean up.\n",
    "\n",
    "Dockerfiles are quite powerful and have many additional commands for adding mount points, exposing ports, setting environmental variables etc.  Be sure to read the [docs](https://docs.docker.com/engine/reference/builder/) for complete details.  For an advanced example of how to add Jupyter notebooks to an image see Appendix B below.\n",
    "\n",
    "It is a [Dockerfile Best Practice](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/) to use a `.dockerignore` file when building images.  Using a `.dockerignore` file you can prevent files an directories from being copied to the images during the build to ensure the images contains only essential files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, notice that our image created with the `dockerfile` has a non-descript name `foo:bar`.  We can use the **`tag`** command to rename an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: no such id: foo\r\n"
     ]
    }
   ],
   "source": [
    "docker tag foo bettername"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hm ... we got an ERROR.  We have to use the full name with `TAG` (i.e. `foo:bar`) when renaming images.  Alternatively, we could use the \"`IMAGE ID`\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "docker tag foo:bar bettername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "bettername          latest              704bbf8f9727        27 seconds ago      1.637 GB\r\n",
      "foo                 bar                 704bbf8f9727        27 seconds ago      1.637 GB\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        8 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        8 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice here that we now have an extra images listed! However, you can confirm with the `df -h` command that the rename/copy we did here didn't actually use any additional hard disk space.  Also, notice that docker automatically gave the image a `TAG` of \"latest\".  We can of course provide a tag explicitly when renaming the image as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "docker tag foo:bar bettername:tagtagtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "foo                 bar                 704bbf8f9727        32 seconds ago      1.637 GB\r\n",
      "bettername          latest              704bbf8f9727        32 seconds ago      1.637 GB\r\n",
      "bettername          tagtagtag           704bbf8f9727        32 seconds ago      1.637 GB\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        8 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        8 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically for personal/local use, image names don't matter too much.  However once we start to share and distribute images, there is an *image naming convention that must be followed* with docker. Use the **`rmi`** command to clean up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such image: foo:bar\r\n",
      "Untagged: bettername:latest\r\n",
      "Untagged: bettername:tagtagtag\r\n",
      "Deleted: sha256:704bbf8f97273ebc0e60a73e65c4ed76254dcbbcd33a90df1baa17d1e663b750\r\n",
      "Deleted: sha256:7038f1e7a8f84cd6079df6f3bf9a820446c9e35132c267d208a76d6b53963878\r\n",
      "Deleted: sha256:049dd8e11f2aff4b65c9049269600826c4630ee3d4da960b1d8851341a149d10\r\n",
      "Deleted: sha256:09796a2db57130d4727fba0ce912d2e25530b60502b02d98ecbd37335164d9f7\r\n"
     ]
    }
   ],
   "source": [
    "docker rmi foo:bar; \n",
    "docker rmi bettername:latest;\n",
    "docker rmi bettername:tagtagtag;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        8 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        8 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this section we learned how create new images from existing containers using the `docker` command **`commit`**.  Furthermore, using the **`export`**/**`import`** commands with containers, or alternatively **`save`**/**`load`** commands with images, we can move containers and images in and out of `docker` for sharing and backup etc.  To facilitate consisten build process we can use a `Dockerfile` script with the **`build`** command to generate more complex images requiring a more complex configuration.  Finally, we saw how we could use the **`tag`** command to rename/copy our images and the **`rmi`** command to delete images.\n",
    "\n",
    "### Exercises\n",
    "At this point you should be comfortable launching containers, looking a logs, attaching standard output, creating your own images, deleting images, renaming images etc etc.  Here are a few suggestions to investigate further: \n",
    "0. Use the `Dockerfile` command **`ENV`** to create an images with predefined environmental variables\n",
    "1. Use the `docker` command **`inspect`** to have a look at what enviroment variables are defined in an image \n",
    "2. Suppose you add enviromental variables `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`, and `AWS_S3_BUCKET` when building your `docker` image.<br /> What problems might arrise when sharing this `docker` image with others??\n",
    "3. What is the difference between **`ADD`** and **`COPY`** commands in a `Dockerfile`?\n",
    "4. What is the difference between the **`ENTRYPOINT`** and **`CMD`** commands in a `Dockerfile`?\n",
    "5. What is the default `entrypoint` for a container?\n",
    "6. What does the [**`VOLUME`**](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#/volume) command do and when should it be used?\n",
    "7. How does the [**`WORKDIR`**](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#/workdir) command differ from the **`VOLUME`**?\n",
    "8. According to the *Docker Best Practices* which is better??\n",
    "\n",
    "    `RUN apt-get install -y automake`<br />\n",
    "    `RUN apt-get install -y build-essential`\n",
    "\n",
    "OR\n",
    "\n",
    "    RUN apt-get install -y automake build-essential\n",
    "\n",
    "\n",
    "#### Food for Thought: \n",
    "Is it possible to automate container build using a `git` hook?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Image Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:09:56 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --rm nvidia/cuda:8.0-cudnn5-devel nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:11:30 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run nvidia/cuda:8.0-cudnn5-devel nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e310b48e17f9\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:16:36 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start -i e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:17:01 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start -a e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: Container e310b48e17f97c1103c53a3c91d008581019782cb523462c68dbbcebc4043e54 is not running\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker exec e310b48e17f9 echo \"hello from nvidia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\r\n",
      "Pulling repository docker.io/library/e310b48e17f9\r\n",
      "nvidia-docker | 2016/11/12 21:29:35 Error: image library/e310b48e17f9:latest not found\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --interactive --tty -d e310b48e17f9 /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "e310b48e17f9        nvidia/cuda:8.0-cudnn5-devel   \"nvidia-smi\"        20 minutes ago      Exited (0) 14 minutes ago                       backstabbing_engelbart\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        3 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        3 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        4 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1868fbf74e0b9e613b4a41760f2b31257c280624a3b70d4705d4cb8c4f519150\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --interactive --tty -d nvidia/cuda:8.0-cudnn5-devel /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS               NAMES\r\n",
      "1868fbf74e0b        nvidia/cuda:8.0-cudnn5-devel   \"/bin/bash\"         16 seconds ago      Up 15 seconds                           stupefied_bhabha\r\n"
     ]
    }
   ],
   "source": [
    "docker ps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUPTI\r\n",
      "Debugger\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker exec 1868fbf74e0b ls /usr/local/cuda/extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "Here are some installation details for getting docker and nvidia-docker up and running from scratch ...\n",
    "\n",
    "### Step 0 - GPU Driver\n",
    "In order to get GPU access with in docker/nvidia-docker we need to make sure that the NVIDIA driver is available on the host system.  It's possible to obtain the appropriate device driver from either the standard [driver download](http://www.nvidia.com/download/index.aspx) page or via [CUDA installation](https://developer.nvidia.com/cuda-downloads).\n",
    "\n",
    "### Step 1 - Docker Install\n",
    "Once the NVIDIA device driver has been successfully installed, we need to install `docker` it self. The installation of docker is quite simple but it is just slightly different for each OS.  The steps for `docker` installation on Ubuntu 14.04 can be found [here](https://docs.docker.com/engine/installation/linux/ubuntulinux/).  Don't worry, the [`docker` docs](https://docs.docker.com/) have install instructions for many other operating systems including RedHat, CentOS, Debian, and so on.\n",
    "\n",
    "Docker provides an official installation script via https://get.docker.com which can accessed via command-line using \"`wget -qO-`\" or \"`curl -sSL`\"\n",
    "\n",
    "### Step 2 - NVIDIA Docker \n",
    "The final configuration step is to obtain the `nvidia-docker` plugin which properly exposes the GPU hardware and drivers for `docker` containers.  Official installation instructions for `nvidia-docker` for Ubuntu, CentOS, and other distributions can be found [here](https://github.com/NVIDIA/nvidia-docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix B\n",
    "Here is the dockerfile for adding Jupyter Notebooks to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create config file for jupyter (file copied by Dockerfile)\n",
    "cat << LINES > jupyter_notebook_config.py\n",
    "c.NotebookApp.ip = '*'\n",
    "c.NotebookApp.port = 8888\n",
    "c.NotebookApp.open_browser = False\n",
    "LINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create run hook for launch (file copied by Dockerfile)\n",
    "cat << LINES > jupyter_notebook_config.py\n",
    "#!/bin/bash\n",
    "jupyter notebook \"$@\"\n",
    "LINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat << LINES > Dockerfile\n",
    "FROM <YOUR/IMAGE:HERE>\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "        libzmq3-dev \\\n",
    "        python-dev \\\n",
    "        python-matplotlib \\\n",
    "        python-pandas \\\n",
    "        python-pip \\\n",
    "        python-sklearn && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip install \\\n",
    "        ipykernel \\\n",
    "        jupyter && \\\n",
    "    python -m ipykernel.kernelspec\n",
    "\n",
    "COPY jupyter_notebook_config.py /root/.jupyter/\n",
    "\n",
    "COPY jupyter.sh /usr/local/bin\n",
    "\n",
    "WORKDIR /data\n",
    "VOLUME /data\n",
    "\n",
    "EXPOSE 8888\n",
    "\n",
    "CMD [\"jupyter.sh\"]\n",
    "\n",
    "LINES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
