{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/docker_logo_1.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably fair to say that access to servers has never been easier.  With platforms such as AWS, Azure, and Google GCE we can now launch on-demand servers of all varieties and configurations.  This programable infrastructure (IaaS) help companies, agencies, and institutions maintain agility as market and mission pressures evolve. However, even with the rise of IaaS, application packaging, configuration, and composition has not advanced despite considerable efforts in configuration management.  This is where [`docker`](https://www.docker.com/) comes in.  \n",
    "\n",
    "`Docker` is not about full virtualization but rather about the ease of packaging and running applications using [Linux containers](https://en.wikipedia.org/wiki/LXC).  The idea is that `docker` containers wrap a piece of software or application in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries (i.e. anything that can be installed on a server). This guarantees that the software will always run the same everywhere, regardless of the OS/compute environment running the container. Docker also provides portable Linux deployment such that containers can be run on any Linux system with kernel is 3.10 or later.  All major Linux distros have supported Docker since 2014.  While no doubt containers and virtual machines have similar resource isolation and allocation benefits, the architectural approach of Linux containers allows containerized applications to be more portable and efficient.\n",
    "\n",
    "At NVIDIA, we use containers in a variety of ways including development, testing, benchmarking, and of course in production as the mechanism for deploying deep learning frameworks. Using [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker), a light-weight `docker` plugin, we can develop and prototype GPU applications on a workstation, and then deploy those applications anywhere that supports GPU containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In the interest of time, we've already configured docker and nvidia-docker. If you're interested in setup details, see Appendix A at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Contact\n",
    "\n",
    "The simplest way to interact with `docker` is probably to just ask for the version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 1.12.3, build 6b644ec\r\n"
     ]
    }
   ],
   "source": [
    "docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask `nvidia-docker` for the version information too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 1.12.3, build 6b644ec\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `nvidia-docker` invocation here was simply \"pass through\" to `docker` command itself.\n",
    "\n",
    "Next best way to get familiar with docker command line is to ask for `--help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: docker [OPTIONS] COMMAND [arg...]\r\n",
      "       docker [ --help | -v | --version ]\r\n",
      "\r\n",
      "A self-sufficient runtime for containers.\r\n",
      "\r\n",
      "Options:\r\n",
      "\r\n",
      "  --config=~/.docker              Location of client config files\r\n",
      "  -D, --debug                     Enable debug mode\r\n",
      "  -H, --host=[]                   Daemon socket(s) to connect to\r\n",
      "  -h, --help                      Print usage\r\n",
      "  -l, --log-level=info            Set the logging level\r\n",
      "  --tls                           Use TLS; implied by --tlsverify\r\n",
      "  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\r\n",
      "  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\r\n",
      "  --tlskey=~/.docker/key.pem      Path to TLS key file\r\n",
      "  --tlsverify                     Use TLS and verify the remote\r\n",
      "  -v, --version                   Print version information and quit\r\n",
      "\r\n",
      "Commands:\r\n",
      "    attach    Attach to a running container\r\n",
      "    build     Build an image from a Dockerfile\r\n",
      "    commit    Create a new image from a container's changes\r\n",
      "    cp        Copy files/folders between a container and the local filesystem\r\n",
      "    create    Create a new container\r\n",
      "    diff      Inspect changes on a container's filesystem\r\n",
      "    events    Get real time events from the server\r\n",
      "    exec      Run a command in a running container\r\n",
      "    export    Export a container's filesystem as a tar archive\r\n",
      "    history   Show the history of an image\r\n",
      "    images    List images\r\n",
      "    import    Import the contents from a tarball to create a filesystem image\r\n",
      "    info      Display system-wide information\r\n",
      "    inspect   Return low-level information on a container, image or task\r\n",
      "    kill      Kill one or more running containers\r\n",
      "    load      Load an image from a tar archive or STDIN\r\n",
      "    login     Log in to a Docker registry.\r\n",
      "    logout    Log out from a Docker registry.\r\n",
      "    logs      Fetch the logs of a container\r\n",
      "    network   Manage Docker networks\r\n",
      "    node      Manage Docker Swarm nodes\r\n",
      "    pause     Pause all processes within one or more containers\r\n",
      "    port      List port mappings or a specific mapping for the container\r\n",
      "    ps        List containers\r\n",
      "    pull      Pull an image or a repository from a registry\r\n",
      "    push      Push an image or a repository to a registry\r\n",
      "    rename    Rename a container\r\n",
      "    restart   Restart a container\r\n",
      "    rm        Remove one or more containers\r\n",
      "    rmi       Remove one or more images\r\n",
      "    run       Run a command in a new container\r\n",
      "    save      Save one or more images to a tar archive (streamed to STDOUT by default)\r\n",
      "    search    Search the Docker Hub for images\r\n",
      "    service   Manage Docker services\r\n",
      "    start     Start one or more stopped containers\r\n",
      "    stats     Display a live stream of container(s) resource usage statistics\r\n",
      "    stop      Stop one or more running containers\r\n",
      "    swarm     Manage Docker Swarm\r\n",
      "    tag       Tag an image into a repository\r\n",
      "    top       Display the running processes of a container\r\n",
      "    unpause   Unpause all processes within one or more containers\r\n",
      "    update    Update configuration of one or more containers\r\n",
      "    version   Show the Docker version information\r\n",
      "    volume    Manage Docker volumes\r\n",
      "    wait      Block until a container stops, then print its exit code\r\n",
      "\r\n",
      "Run 'docker COMMAND --help' for more information on a command.\r\n"
     ]
    }
   ],
   "source": [
    "docker --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of `docker` command line interactions is: \n",
    "\n",
    "`docker [OPTIONS] COMMAND [arg...]` \n",
    "\n",
    "and as the help display shows there are a lot of commands to choose from.  Don't worry, much like a big city, once we become more familiar with these commands the list won't seem so big.  We can start to drill down and get help that is specific to each command.  For example, one of the most useful `docker` commands is **`images`** which list all local containers on the host that `docker` knows about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:\tdocker images [OPTIONS] [REPOSITORY[:TAG]]\r\n",
      "\r\n",
      "List images\r\n",
      "\r\n",
      "Options:\r\n",
      "  -a, --all             Show all images (default hides intermediate images)\r\n",
      "      --digests         Show digests\r\n",
      "  -f, --filter value    Filter output based on conditions provided (default [])\r\n",
      "      --format string   Pretty-print images using a Go template\r\n",
      "      --help            Print usage\r\n",
      "      --no-trunc        Don't truncate output\r\n",
      "  -q, --quiet           Only show numeric IDs\r\n"
     ]
    }
   ],
   "source": [
    "docker images --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, lets now ask `docker` about the what container images are available locally on the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        5 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        5 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the output specifies three container images with some general metadata associated with each one.  First you'll notice that the images are quite large on average (~ 2GB) and that each image is associated with a unique ID hash.  When containers are created (i.e. via the **`create`** command) they are created from images.  There is no limit to the number of containers we can create from an images so it is important that `docker` associates UUIDs for each image and container.  Notice the `REPOSITORY` and `TAG` columns here specify more human readable image labels.  The repository loosely coresponds to the image name (i.e. url) and just as in the version control system [GIT](https://git-scm.com/) images can be modified and \"tagged\" rather than explicitly changing the image name for each image version. \n",
    "\n",
    "Here we have the \"`nvidia/cuda`\" container with ID `c54a2cc56cbb` and is tagged as the \"`latest`\" version of the image (i.e. most current).  The deep learning library [`cuDNN`](https://developer.nvidia.com/cudnn) was added to the image and a new image was created under the same name but tagged appropriately as \"`8.0-cudnn5-devel`\".\n",
    "\n",
    "You're probably wondering already \"*where does docker store these containers?*\".  In general, docker works in `/var/lib/docker` and images are stored in `image` subdirectory.  For more information and details about where and how docker stores images on the host machine, see [here](https://stackoverflow.com/questions/19234831/where-are-docker-images-stored-on-the-host-machine#). \n",
    "\n",
    "For now just know that docker works with \"images\" and all containers are created from these images.  We will go into all the details about creating and modifying images etc in just a bit.  But first, lets actually kick around some containers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first lets have docker list *all* containers using the **`ps`** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, don't forget you can get help for each command with `docker [COMMAND] --help`.  Use this to get additional details on the **`ps`** command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now use the docker command **`create`** to initialize a container from the `nvidia/cuda:latest` image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9a12d94b12fd4e3de47a3ee3bab4d771d80b6895b575dc2c1ad257cb0f3d121f\r\n"
     ]
    }
   ],
   "source": [
    "docker create nvidia/cuda:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responce we recieved is a sha256 UUID for the generated container and listing `docker` containers again we see this new container now listed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS               NAMES\r\n",
      "9a12d94b12fd        nvidia/cuda:latest   \"/bin/bash\"         45 seconds ago      Created                                 lonely_easley\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that the container is not actually doing anything right now.  We've only \"stamped\" out a container from an image -- the container is not running at this point.  Were the container active the `STATUS` would read \"running\".  OK, so what is the container doing there?  Well the answer is \"nothing\".  Think about when we enter commands on the command-line -- each time we hit enter we implicitly specify that we would like that command to be executed immediately.  You can think of containers as a command that has not yet executed.  This command is wrapped up in the container and has all the resources (libraries etc) needed for successful execution.  Speaking of which, lets actually run this container ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\r\n",
      "Pulling repository docker.io/library/9a12d94b12fd\r\n",
      "nvidia-docker | 2016/11/30 01:01:55 Error: image library/9a12d94b12fd:latest not found\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run 9a12d94b12fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hm ... using the **`run`** command seemed like a good guess.  Lets try the **`start`** command instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9a12d94b12fd\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start 9a12d94b12fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks better. Using the **`start`** command docker returned the sha256 UUID.  Lets have a look at the docker containers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS                     PORTS               NAMES\r\n",
      "9a12d94b12fd        nvidia/cuda:latest   \"/bin/bash\"         About an hour ago   Exited (0) 6 minutes ago                       lonely_easley\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the status says \"Exited (0) ...\".  Notice that the command (i.e. entry point) is `/bin/bash`.  When the **`start`** was issued the \"COMMAND\" was executed and by definition *bash command language interpreter that executes commands read from the standard input or from a file*.  However, there were no commands to execute from standard input!  Containers can have other entry points -- the reason `/bin/bash` is used most often is that it allows the container to act more generically as a shell so we can send it additional instructions. Note that all containers have a default entrypoint of \"`/bin/sh -c`\" unless otherwise specified.\n",
    "\n",
    "Here our hands are tied with what the container will do.  Each time we issue the **`start`** command the container executes the entrypoint \"`/bin/bash`\" and since there is nothing on standard input the container simply exits.  This is where the **`run`** command comes in.\n",
    "\n",
    "Instead of creating and starting a container explicitly we can use the **`run`** command to exectue a command within a particular image via creating a container from that image with the appropriate entrypoint.  Lets issue a **`run`** command passing the image ID of the \"`nvidia/cuda:latest`\" image as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "nvidia-docker run 367795fb1051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the command **`start`** takes a container ID as the argument while the **`run`** command takes an image ID.  Lets have a look at the containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED              STATUS                          PORTS               NAMES\r\n",
      "b79dee54ffe1        367795fb1051         \"/bin/bash\"         About a minute ago   Exited (0) About a minute ago                       amazing_euler\r\n",
      "ff763f6099b1        367795fb1051         \"/bin/bash\"         3 minutes ago        Exited (0) 2 minutes ago                            suspicious_pike\r\n",
      "9a12d94b12fd        nvidia/cuda:latest   \"/bin/bash\"         2 hours ago          Exited (0) 18 minutes ago                           lonely_easley\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an additional container both from image `367795fb1051` which have exited.  At this point the **`run`** command has done exactly what the **`start`** command has done (i.e. started a container which executed the entrypoint and exited).  However, the docker **`run`** command allows us to pass an alternative command to the container (`docker run --help`).  Lets try to pass an alternative instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 30 01:49:50 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   26C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run 367795fb1051 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally!  Just to be clear, the `nvidia-smi` command was exectued within the container -- not the host.  Lets have a look at the containers yet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "2747af7b70a0        367795fb1051         \"nvidia-smi\"        19 seconds ago      Exited (0) 18 seconds ago                       berserk_goldwasser\r\n",
      "b79dee54ffe1        367795fb1051         \"/bin/bash\"         7 minutes ago       Exited (0) 7 minutes ago                        amazing_euler\r\n",
      "ff763f6099b1        367795fb1051         \"/bin/bash\"         9 minutes ago       Exited (0) 9 minutes ago                        suspicious_pike\r\n",
      "9a12d94b12fd        nvidia/cuda:latest   \"/bin/bash\"         2 hours ago         Exited (0) 24 minutes ago                       lonely_easley\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new container from image `367795fb1051` but the \"COMMAND\" has been set to `nvidia-smi` as instructed by our **`run`** command.  Just for kicks lets issue a **`start`** command to this new container.  Each container gets a uuid that will change every time this lab is run so make sure to replace the container ID in the command below with the appropriate container ID listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2747af7b70a0\r\n"
     ]
    }
   ],
   "source": [
    "docker start 2747af7b70a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wait a minute, where is our output??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "2747af7b70a0        367795fb1051         \"nvidia-smi\"        11 minutes ago      Exited (0) 3 seconds ago                        berserk_goldwasser\r\n",
      "b79dee54ffe1        367795fb1051         \"/bin/bash\"         19 minutes ago      Exited (0) 19 minutes ago                       amazing_euler\r\n",
      "ff763f6099b1        367795fb1051         \"/bin/bash\"         20 minutes ago      Exited (0) 20 minutes ago                       suspicious_pike\r\n",
      "9a12d94b12fd        nvidia/cuda:latest   \"/bin/bash\"         2 hours ago         Exited (0) 36 minutes ago                       lonely_easley\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough when we check the docker container status it has status of \"Exited (0) 10 seconds ago ...\" which means that the **`start`** command did indeed start the container. The long story short is that the **`run`** command automatically provides the standard output from the command specified where as **`start`** does not forward the stdout by default -- we have to explicitly ask.  According to the help section for the **`start`** command, the option \"`--attach`\" attaches STDOUT/STDERR and forward signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 30 02:10:54 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   26C    P8    17W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "docker start --attach 2747af7b70a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo!\n",
    "\n",
    "A few final words on starting and running containers. Keep an eye out on the container list when using the **`run`** command as each invocation creates a *new* image.  There is no problem having many (many) container stitting around but eventually it creates clutter.  Remember, containers are ment to be light-weight and disposable.  To that end lets clean up our containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2747af7b70a0\r\n",
      "b79dee54ffe1\r\n",
      "ff763f6099b1\r\n",
      "9a12d94b12fd\r\n"
     ]
    }
   ],
   "source": [
    "# generate a list of container ID from the docker ps command\n",
    "docker ps -a | awk '{print $1}' | tail -n +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2747af7b70a0\r\n",
      "b79dee54ffe1\r\n",
      "ff763f6099b1\r\n",
      "9a12d94b12fd\r\n"
     ]
    }
   ],
   "source": [
    "# for each container ID use the docker \"rm\" command to remove/delete the container\n",
    "for cid in $(docker ps -a | awk '{print $1}' | tail -n +2);do docker rm $cid; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "# check docker containers one last time\n",
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All clean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovery Requires Experimentation\n",
    "Make sure that you're comfortable creating containers and executing commands before moving forward.  Docker is quite forgiving so do be afraid to try lots of different things out while you explore.  A few ideas of things to try are \n",
    "1. Get the container to ping google\n",
    "2. Maybe try `ifconfig` inside of the container\n",
    "3. What does the container return when asked for disk usage (i.e `df -h`)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PING www.google.com (172.217.5.4) 56(84) bytes of data.\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=1 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=2 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=3 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=4 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=5 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=6 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=7 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=8 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=9 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=10 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=11 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=12 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=13 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=14 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=15 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=16 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=17 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=18 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=19 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=20 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=21 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=22 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=23 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=24 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=25 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=26 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=27 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=28 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=29 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=30 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=31 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=32 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=33 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=34 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=35 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=36 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=37 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=38 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=39 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=40 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=41 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=42 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=43 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=44 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=45 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=46 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=47 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=48 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=49 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=50 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=51 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=52 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=53 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=54 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=55 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=56 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=57 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=58 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=59 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=60 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=61 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=62 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=63 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=64 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=65 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=66 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=67 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=68 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=69 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=70 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=71 Time to live exceeded\r\n",
      "From ip-172-17-0-1.ec2.internal (172.17.0.1) icmp_seq=72 Time to live exceeded\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run 367795fb1051 ping -t1 www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving Deeper into Containers and Image Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:09:56 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --rm nvidia/cuda:8.0-cudnn5-devel nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:11:30 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run nvidia/cuda:8.0-cudnn5-devel nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "e310b48e17f9        nvidia/cuda:8.0-cudnn5-devel   \"nvidia-smi\"        12 seconds ago      Exited (0) 11 seconds ago                       backstabbing_engelbart\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e310b48e17f9\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:\tdocker start [OPTIONS] CONTAINER [CONTAINER...]\r\n",
      "\r\n",
      "Start one or more stopped containers\r\n",
      "\r\n",
      "Options:\r\n",
      "  -a, --attach               Attach STDOUT/STDERR and forward signals\r\n",
      "      --detach-keys string   Override the key sequence for detaching a container\r\n",
      "      --help                 Print usage\r\n",
      "  -i, --interactive          Attach container's STDIN\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:16:36 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start -i e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:17:01 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start -a e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: Container e310b48e17f97c1103c53a3c91d008581019782cb523462c68dbbcebc4043e54 is not running\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker exec e310b48e17f9 echo \"hello from nvidia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\r\n",
      "Pulling repository docker.io/library/e310b48e17f9\r\n",
      "nvidia-docker | 2016/11/12 21:29:35 Error: image library/e310b48e17f9:latest not found\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --interactive --tty -d e310b48e17f9 /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "e310b48e17f9        nvidia/cuda:8.0-cudnn5-devel   \"nvidia-smi\"        20 minutes ago      Exited (0) 14 minutes ago                       backstabbing_engelbart\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        3 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        3 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        4 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1868fbf74e0b9e613b4a41760f2b31257c280624a3b70d4705d4cb8c4f519150\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --interactive --tty -d nvidia/cuda:8.0-cudnn5-devel /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS               NAMES\r\n",
      "1868fbf74e0b        nvidia/cuda:8.0-cudnn5-devel   \"/bin/bash\"         16 seconds ago      Up 15 seconds                           stupefied_bhabha\r\n"
     ]
    }
   ],
   "source": [
    "docker ps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUPTI\r\n",
      "Debugger\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker exec 1868fbf74e0b ls /usr/local/cuda/extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "Here are some installation details for getting docker up and running from scratch ...\n",
    "\n",
    "### Step 0 - GPU Driver\n",
    "In order to get GPU access with in docker/nvidia-docker we need to make sure that the NVIDIA driver is available on the host system.  It's possible to obtain the appropriate device driver from either the standard [driver download](http://www.nvidia.com/download/index.aspx) page or via [CUDA installation](https://developer.nvidia.com/cuda-downloads).\n",
    "\n",
    "### Step 1 - Docker Install\n",
    "Once the NVIDIA device driver has been successfully installed, we need to install `docker` it self. The installation of docker is quite simple but it is just slightly different for each OS.  The steps for `docker` installation on Ubuntu 14.04 can be found [here](https://docs.docker.com/engine/installation/linux/ubuntulinux/).  Don't worry, the [`docker` docs](https://docs.docker.com/) have install instructions for many other operating systems including RedHat, CentOS, Debian, and so on.\n",
    "\n",
    "Docker provides an official installation script via https://get.docker.com which can accessed via command-line using \"`wget -qO-`\" or \"`curl -sSL`\"\n",
    "\n",
    "### Step 2 - NVIDIA Docker \n",
    "The final configuration step is to obtain the `nvidia-docker` plugin which properly exposes the GPU hardware and drivers for `docker` containers.  Official installation instructions for `nvidia-docker` for Ubuntu, CentOS, and other distributions can be found [here](https://github.com/NVIDIA/nvidia-docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
