{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/docker_logo_1.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably fair to say that access to servers has never been easier.  With platforms such as AWS, Azure, and Google GCE we can now launch on-demand servers of all varieties and configurations.  This programable infrastructure (IaaS) help companies, agencies, and institutions maintain agility as market and mission pressures evolve. However, even with the rise of IaaS, application packaging, configuration, and composition has not advanced despite considerable efforts in configuration management.  This is where [`docker`](https://www.docker.com/) comes in.  \n",
    "\n",
    "`Docker` is not about full virtualization but rather about the ease of packaging and running applications using [Linux containers](https://en.wikipedia.org/wiki/LXC).  The idea is that `docker` containers wrap a piece of software or application in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries (i.e. anything that can be installed on a server). This guarantees that the software will always run the same everywhere, regardless of the OS/compute environment running the container. Docker also provides portable Linux deployment such that containers can be run on any Linux system with kernel is 3.10 or later.  All major Linux distros have supported Docker since 2014.  While no doubt containers and virtual machines have similar resource isolation and allocation benefits, the architectural approach of Linux containers allows containerized applications to be more portable and efficient.\n",
    "\n",
    "At NVIDIA, we use containers in a variety of ways including development, testing, benchmarking, and of course in production as the mechanism for deploying deep learning frameworks. Using [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker), a light-weight `docker` plugin, we can develop and prototype GPU applications on a workstation, and then deploy those applications anywhere that supports GPU containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In the interest of time, we've already configured docker and nvidia-docker. If you're interested in setup details, see Appendix A at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Contact\n",
    "\n",
    "The simplest way to interact with `docker` is probably to just ask for the version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 1.12.3, build 6b644ec\r\n"
     ]
    }
   ],
   "source": [
    "docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask `nvidia-docker` for the version information too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 1.12.3, build 6b644ec\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `nvidia-docker` invocation here was simply \"pass through\" to `docker` command itself.\n",
    "\n",
    "Next best way to get familiar with docker command line is to ask for `--help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: docker [OPTIONS] COMMAND [arg...]\r\n",
      "       docker [ --help | -v | --version ]\r\n",
      "\r\n",
      "A self-sufficient runtime for containers.\r\n",
      "\r\n",
      "Options:\r\n",
      "\r\n",
      "  --config=~/.docker              Location of client config files\r\n",
      "  -D, --debug                     Enable debug mode\r\n",
      "  -H, --host=[]                   Daemon socket(s) to connect to\r\n",
      "  -h, --help                      Print usage\r\n",
      "  -l, --log-level=info            Set the logging level\r\n",
      "  --tls                           Use TLS; implied by --tlsverify\r\n",
      "  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\r\n",
      "  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\r\n",
      "  --tlskey=~/.docker/key.pem      Path to TLS key file\r\n",
      "  --tlsverify                     Use TLS and verify the remote\r\n",
      "  -v, --version                   Print version information and quit\r\n",
      "\r\n",
      "Commands:\r\n",
      "    attach    Attach to a running container\r\n",
      "    build     Build an image from a Dockerfile\r\n",
      "    commit    Create a new image from a container's changes\r\n",
      "    cp        Copy files/folders between a container and the local filesystem\r\n",
      "    create    Create a new container\r\n",
      "    diff      Inspect changes on a container's filesystem\r\n",
      "    events    Get real time events from the server\r\n",
      "    exec      Run a command in a running container\r\n",
      "    export    Export a container's filesystem as a tar archive\r\n",
      "    history   Show the history of an image\r\n",
      "    images    List images\r\n",
      "    import    Import the contents from a tarball to create a filesystem image\r\n",
      "    info      Display system-wide information\r\n",
      "    inspect   Return low-level information on a container, image or task\r\n",
      "    kill      Kill one or more running containers\r\n",
      "    load      Load an image from a tar archive or STDIN\r\n",
      "    login     Log in to a Docker registry.\r\n",
      "    logout    Log out from a Docker registry.\r\n",
      "    logs      Fetch the logs of a container\r\n",
      "    network   Manage Docker networks\r\n",
      "    node      Manage Docker Swarm nodes\r\n",
      "    pause     Pause all processes within one or more containers\r\n",
      "    port      List port mappings or a specific mapping for the container\r\n",
      "    ps        List containers\r\n",
      "    pull      Pull an image or a repository from a registry\r\n",
      "    push      Push an image or a repository to a registry\r\n",
      "    rename    Rename a container\r\n",
      "    restart   Restart a container\r\n",
      "    rm        Remove one or more containers\r\n",
      "    rmi       Remove one or more images\r\n",
      "    run       Run a command in a new container\r\n",
      "    save      Save one or more images to a tar archive (streamed to STDOUT by default)\r\n",
      "    search    Search the Docker Hub for images\r\n",
      "    service   Manage Docker services\r\n",
      "    start     Start one or more stopped containers\r\n",
      "    stats     Display a live stream of container(s) resource usage statistics\r\n",
      "    stop      Stop one or more running containers\r\n",
      "    swarm     Manage Docker Swarm\r\n",
      "    tag       Tag an image into a repository\r\n",
      "    top       Display the running processes of a container\r\n",
      "    unpause   Unpause all processes within one or more containers\r\n",
      "    update    Update configuration of one or more containers\r\n",
      "    version   Show the Docker version information\r\n",
      "    volume    Manage Docker volumes\r\n",
      "    wait      Block until a container stops, then print its exit code\r\n",
      "\r\n",
      "Run 'docker COMMAND --help' for more information on a command.\r\n"
     ]
    }
   ],
   "source": [
    "docker --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of `docker` command line interactions is: \n",
    "\n",
    "`docker [OPTIONS] COMMAND [arg...]` \n",
    "\n",
    "and as the help display shows there are a lot of commands to choose from.  Don't worry, much like a big city, once we become more familiar with these commands the list won't seem so big.  We can start to drill down and get help that is specific to each command.  For example, one of the most useful `docker` commands is **`images`** which list all local containers on the host that `docker` knows about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:\tdocker images [OPTIONS] [REPOSITORY[:TAG]]\r\n",
      "\r\n",
      "List images\r\n",
      "\r\n",
      "Options:\r\n",
      "  -a, --all             Show all images (default hides intermediate images)\r\n",
      "      --digests         Show digests\r\n",
      "  -f, --filter value    Filter output based on conditions provided (default [])\r\n",
      "      --format string   Pretty-print images using a Go template\r\n",
      "      --help            Print usage\r\n",
      "      --no-trunc        Don't truncate output\r\n",
      "  -q, --quiet           Only show numeric IDs\r\n"
     ]
    }
   ],
   "source": [
    "docker images --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, lets now ask `docker` about the what container images are available locally on the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        5 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        5 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        5 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the output specifies three container images with some general metadata associated with each one.  First you'll notice that the images are quite large on average (~ 2GB) and that each image is associated with a unique ID hash.  When containers are created (i.e. via the **`create`** command) they are created from images.  There is no limit to the number of containers we can create from an images so it is important that `docker` associates UUIDs for each image and container.  Notice the `REPOSITORY` and `TAG` columns here specify more human readable image labels.  The repository loosely coresponds to the image name (i.e. url) and just as in the version control system [GIT](https://git-scm.com/) images can be modified and \"tagged\" rather than explicitly changing the image name for each image version. \n",
    "\n",
    "Here we have the \"`nvidia/cuda`\" container with ID `c54a2cc56cbb` and is tagged as the \"`latest`\" version of the image (i.e. most current).  The deep learning library [`cuDNN`](https://developer.nvidia.com/cudnn) was added to the image and a new image was created under the same name but tagged appropriately as \"`8.0-cudnn5-devel`\".\n",
    "\n",
    "You're probably wondering already \"*where does docker store these containers?*\".  In general, docker works in `/var/lib/docker` and images are stored in `image` subdirectory.  For more information and details about where and how docker stores images on the host machine, see [here](https://stackoverflow.com/questions/19234831/where-are-docker-images-stored-on-the-host-machine#). \n",
    "\n",
    "For now just know that docker works with \"images\" and all containers are created from these images.  We will go into all the details about creating and modifying images etc in just a bit.  But first, lets actually kick around some containers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first lets have docker list *all* containers using the **`ps`** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, don't forget you can get help for each command with `docker [COMMAND] --help`.  Use this to get additional details on the **`ps`** command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now use the docker command **`create`** to initialize a container from the `nvidia/cuda:latest` image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9a12d94b12fd4e3de47a3ee3bab4d771d80b6895b575dc2c1ad257cb0f3d121f\r\n"
     ]
    }
   ],
   "source": [
    "docker create nvidia/cuda:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responce we recieved is a sha256 UUID for the generated container and listing `docker` containers again we see this new container now listed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS               NAMES\r\n",
      "9a12d94b12fd        nvidia/cuda:latest   \"/bin/bash\"         45 seconds ago      Created                                 lonely_easley\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that the container is not actually doing anything right now.  We've only \"stamped\" out a container from an image -- the container is not running at this point.  Were the container active the `STATUS` would read \"running\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:09:56 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --rm nvidia/cuda:8.0-cudnn5-devel nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:11:30 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run nvidia/cuda:8.0-cudnn5-devel nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "e310b48e17f9        nvidia/cuda:8.0-cudnn5-devel   \"nvidia-smi\"        12 seconds ago      Exited (0) 11 seconds ago                       backstabbing_engelbart\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e310b48e17f9\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Usage:\tdocker start [OPTIONS] CONTAINER [CONTAINER...]\r\n",
      "\r\n",
      "Start one or more stopped containers\r\n",
      "\r\n",
      "Options:\r\n",
      "  -a, --attach               Attach STDOUT/STDERR and forward signals\r\n",
      "      --detach-keys string   Override the key sequence for detaching a container\r\n",
      "      --help                 Print usage\r\n",
      "  -i, --interactive          Attach container's STDIN\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:16:36 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start -i e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 21:17:01 2016       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   45C    P8    28W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker start -a e310b48e17f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: Container e310b48e17f97c1103c53a3c91d008581019782cb523462c68dbbcebc4043e54 is not running\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker exec e310b48e17f9 echo \"hello from nvidia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\r\n",
      "Pulling repository docker.io/library/e310b48e17f9\r\n",
      "nvidia-docker | 2016/11/12 21:29:35 Error: image library/e310b48e17f9:latest not found\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --interactive --tty -d e310b48e17f9 /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                      PORTS               NAMES\r\n",
      "e310b48e17f9        nvidia/cuda:8.0-cudnn5-devel   \"nvidia-smi\"        20 minutes ago      Exited (0) 14 minutes ago                       backstabbing_engelbart\r\n"
     ]
    }
   ],
   "source": [
    "docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n",
      "nvidia/cuda         8.0-cudnn5-devel    31582c303549        3 weeks ago         1.776 GB\r\n",
      "nvidia/cuda         latest              367795fb1051        3 weeks ago         1.615 GB\r\n",
      "hello-world         latest              c54a2cc56cbb        4 months ago        1.848 kB\r\n"
     ]
    }
   ],
   "source": [
    "docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1868fbf74e0b9e613b4a41760f2b31257c280624a3b70d4705d4cb8c4f519150\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker run --interactive --tty -d nvidia/cuda:8.0-cudnn5-devel /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS               NAMES\r\n",
      "1868fbf74e0b        nvidia/cuda:8.0-cudnn5-devel   \"/bin/bash\"         16 seconds ago      Up 15 seconds                           stupefied_bhabha\r\n"
     ]
    }
   ],
   "source": [
    "docker ps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUPTI\r\n",
      "Debugger\r\n"
     ]
    }
   ],
   "source": [
    "nvidia-docker exec 1868fbf74e0b ls /usr/local/cuda/extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "Here are some installation details for getting docker up and running from scratch ...\n",
    "\n",
    "### Step 0 - GPU Driver\n",
    "In order to get GPU access with in docker/nvidia-docker we need to make sure that the NVIDIA driver is available on the host system.  It's possible to obtain the appropriate device driver from either the standard [driver download](http://www.nvidia.com/download/index.aspx) page or via [CUDA installation](https://developer.nvidia.com/cuda-downloads).\n",
    "\n",
    "### Step 1 - Docker Install\n",
    "Once the NVIDIA device driver has been successfully installed, we need to install `docker` it self. The installation of docker is quite simple but it is just slightly different for each OS.  The steps for `docker` installation on Ubuntu 14.04 can be found [here](https://docs.docker.com/engine/installation/linux/ubuntulinux/).  Don't worry, the [`docker` docs](https://docs.docker.com/) have install instructions for many other operating systems including RedHat, CentOS, Debian, and so on.\n",
    "\n",
    "Docker provides an official installation script via https://get.docker.com which can accessed via command-line using \"`wget -qO-`\" or \"`curl -sSL`\"\n",
    "\n",
    "### Step 2 - NVIDIA Docker \n",
    "The final configuration step is to obtain the `nvidia-docker` plugin which properly exposes the GPU hardware and drivers for `docker` containers.  Official installation instructions for `nvidia-docker` for Ubuntu, CentOS, and other distributions can be found [here](https://github.com/NVIDIA/nvidia-docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
